{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daeb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae269ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "vec_len = 50\n",
    "seq_len = 20\n",
    "num_epochs = 50\n",
    "label_col = \"Product\"\n",
    "tokens_path = \"Output/tokens.pkl\"\n",
    "labels_path = \"Output/labels.pkl\"\n",
    "data_path = \"Input/complaints.csv\"\n",
    "model_path = \"Output/attention.pth\"\n",
    "vocabulary_path = \"Output/vocabulary.pkl\"\n",
    "embeddings_path = \"Output/embeddings.pkl\"\n",
    "glove_vector_path = \"Input/glove.6B.50d.txt\"\n",
    "text_col_name = \"Consumer complaint narrative\"\n",
    "label_encoder_path = \"Output/label_encoder.pkl\"\n",
    "product_map = {'Vehicle loan or lease': 'vehicle_loan',\n",
    "               'Credit reporting, credit repair services, or other personal consumer reports': 'credit_report',\n",
    "               'Credit card or prepaid card': 'card',\n",
    "               'Money transfer, virtual currency, or money service': 'money_transfer',\n",
    "               'virtual currency': 'money_transfer',\n",
    "               'Mortgage': 'mortgage',\n",
    "               'Payday loan, title loan, or personal loan': 'loan',\n",
    "               'Debt collection': 'debt_collection',\n",
    "               'Checking or savings account': 'savings_account',\n",
    "               'Credit card': 'card',\n",
    "               'Bank account or service': 'savings_account',\n",
    "               'Credit reporting': 'credit_report',\n",
    "               'Prepaid card': 'card',\n",
    "               'Payday loan': 'loan',\n",
    "               'Other financial service': 'others',\n",
    "               'Virtual currency': 'money_transfer',\n",
    "               'Student loan': 'loan',\n",
    "               'Consumer Loan': 'loan',\n",
    "               'Money transfers': 'money_transfer'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce10573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(name, obj):\n",
    "    \"\"\"\n",
    "    Function to save an object as pickle file\n",
    "    \"\"\"\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_file(name):\n",
    "    \"\"\"\n",
    "    Function to load a pickle object\n",
    "    \"\"\"\n",
    "    return pickle.load(open(name, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217b362",
   "metadata": {},
   "source": [
    "## Process glove embeddings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa79027",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(glove_vector_path, \"rt\") as f:\n",
    "    emb = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, embeddings = [], []\n",
    "\n",
    "for item in emb:\n",
    "    vocabulary.append(item.split()[0])\n",
    "    embeddings.append(item.split()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ab05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\"<pad>\", \"<unk>\"] + vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.vstack([np.ones(50, dtype=np.float32), \n",
    "                        np.mean(embeddings, axis=0),\n",
    "                        embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(embeddings_path, embeddings)\n",
    "save_file(vocabulary_path, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5b4ed",
   "metadata": {},
   "source": [
    "## Process text data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0857024",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[text_col_name], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace({label_col: product_map}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2046a04",
   "metadata": {},
   "source": [
    "### Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data[label_col])\n",
    "labels = label_encoder.transform(data[label_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(labels_path, labels)\n",
    "save_file(label_encoder_path, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b68c08",
   "metadata": {},
   "source": [
    "### Process the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = list(data[text_col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49234f",
   "metadata": {},
   "source": [
    "### Convert text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf262880",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [i.lower() for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef1b67",
   "metadata": {},
   "source": [
    "### Remove punctuations except apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [re.sub(r\"[^\\w\\d'\\s]+\", \" \", i) \n",
    "              for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78914b0f",
   "metadata": {},
   "source": [
    "### Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed47512",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24176de",
   "metadata": {},
   "source": [
    "### Remove more than one consecutive instance of 'x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a31a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [re.sub(r'[x]{2,}', \"\", i) for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9005a",
   "metadata": {},
   "source": [
    "### Remove multiple spaces with single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [re.sub(' +', ' ', i) for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2eb78",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3393da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(t) for t in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f29b1b2",
   "metadata": {},
   "source": [
    "### Take the first 20 tokens in each complaint text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [i[:20] if len(i) > 19 else ['<pad>'] * (20 - len(i)) + i \n",
    "          for i in tqdm(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec18f209",
   "metadata": {},
   "source": [
    "### Convert tokens to integer indices from vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd525d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_index(tokens, vocabulary, missing='<unk>'):\n",
    "    \"\"\"\n",
    "    :param tokens: List of word tokens\n",
    "    :param vocabulary: All words in the embeddings\n",
    "    :param missing: Token for words not present in the vocabulary\n",
    "    :return: List of integers representing the word tokens\n",
    "    \"\"\"\n",
    "    idx_token = []\n",
    "    for text in tqdm(tokens):\n",
    "        idx_text = []\n",
    "        for token in text:\n",
    "            if token in vocabulary:\n",
    "                idx_text.append(vocabulary.index(token))\n",
    "            else:\n",
    "                idx_text.append(vocabulary.index(missing))\n",
    "        idx_token.append(idx_text)\n",
    "    return idx_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = token_index(tokens, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e48e10a",
   "metadata": {},
   "source": [
    "### Save the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23952d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(tokens_path, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b005f51",
   "metadata": {},
   "source": [
    "## Create attention model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vec_len, seq_len, n_classes):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.vec_len = vec_len\n",
    "        self.seq_len = seq_len\n",
    "        self.attn_weights = torch.cat([torch.tensor([[0.]]),\n",
    "                                       torch.randn(vec_len, 1) /\n",
    "                                       torch.sqrt(torch.tensor(vec_len))])\n",
    "        self.attn_weights.requires_grad = True\n",
    "        self.attn_weights = nn.Parameter(self.attn_weights)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.linear = nn.Linear(vec_len + 1, n_classes)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        hidden = torch.matmul(input_data, self.attn_weights)\n",
    "        hidden = self.activation(hidden)\n",
    "        attn = self.softmax(hidden)\n",
    "        attn = attn.repeat(1, 1, self.vec_len + 1).reshape(attn.shape[0],\n",
    "                                                           self.seq_len,\n",
    "                                                           self.vec_len + 1)\n",
    "        attn_output = input_data * attn\n",
    "        attn_output = torch.sum(attn_output, axis=1)\n",
    "        output = self.linear(attn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f515b0",
   "metadata": {},
   "source": [
    "## Create PyTorch dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7449d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokens, embeddings, labels):\n",
    "        \"\"\"\n",
    "        :param tokens: List of word tokens\n",
    "        :param embeddings: Word embeddings (from glove)\n",
    "        :param labels: List of labels\n",
    "        \"\"\"\n",
    "        self.tokens = tokens\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb = torch.tensor(self.embeddings[self.tokens[idx], :])\n",
    "        input_ = torch.cat((torch.ones(emb.shape[0],1), emb), dim=1)\n",
    "        return torch.tensor(self.labels[idx]), input_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa32d5",
   "metadata": {},
   "source": [
    "### Function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d51e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, criterion, optimizer, \n",
    "          device, num_epochs, model_path):\n",
    "    \"\"\"\n",
    "    Function to train the model\n",
    "    :param train_loader: Data loader for train dataset\n",
    "    :param valid_loader: Data loader for validation dataset\n",
    "    :param model: Model object\n",
    "    :param criterion: Loss function\n",
    "    :param optimizer: Optimizer\n",
    "    :param device: CUDA or CPU\n",
    "    :param num_epochs: Number of epochs\n",
    "    :param model_path: Path to save the model\n",
    "    \"\"\"\n",
    "    best_loss = 1e8\n",
    "    for i in range(num_epochs):\n",
    "        print(f\"Epoch {i+1} of {num_epochs}\")\n",
    "        valid_loss, train_loss = [], []\n",
    "        model.train()\n",
    "        # Train loop\n",
    "        for batch_labels, batch_data in tqdm(train_loader):\n",
    "            # Move data to GPU if available\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            batch_data = batch_data.to(device)\n",
    "            # Forward pass\n",
    "            batch_output = model(batch_data)\n",
    "            batch_output = torch.squeeze(batch_output)\n",
    "            # Calculate loss\n",
    "            loss = criterion(batch_output, batch_labels)\n",
    "            train_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Gradient update step\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        # Validation loop\n",
    "        for batch_labels, batch_data in tqdm(valid_loader):\n",
    "            # Move data to GPU if available\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            batch_data = batch_data.to(device)\n",
    "            # Forward pass\n",
    "            batch_output = model(batch_data)\n",
    "            batch_output = torch.squeeze(batch_output)\n",
    "            # Calculate loss\n",
    "            loss = criterion(batch_output, batch_labels)\n",
    "            valid_loss.append(loss.item())\n",
    "        t_loss = np.mean(train_loss)\n",
    "        v_loss = np.mean(valid_loss)\n",
    "        print(f\"Train Loss: {t_loss}, Validation Loss: {v_loss}\")\n",
    "        if v_loss < best_loss:\n",
    "            best_loss = v_loss\n",
    "            # Save model if validation loss improves\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Best Validation Loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7227423",
   "metadata": {},
   "source": [
    "### Function to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Function to test the model\n",
    "    :param test_loader: Data loader for test dataset\n",
    "    :param model: Model object\n",
    "    :param criterion: Loss function\n",
    "    :param device: CUDA or CPU\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    test_accu = []\n",
    "    for batch_labels, batch_data in tqdm(test_loader):\n",
    "        # Move data to device\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        batch_data = batch_data.to(device)\n",
    "        # Forward pass\n",
    "        batch_output = model(batch_data)\n",
    "        batch_output = torch.squeeze(batch_output)\n",
    "        # Calculate loss\n",
    "        loss = criterion(batch_output, batch_labels)\n",
    "        test_loss.append(loss.item())\n",
    "        batch_preds = torch.argmax(batch_output, axis=1)\n",
    "        # Move predictions to CPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch_labels = batch_labels.cpu()\n",
    "            batch_preds = batch_preds.cpu()\n",
    "        # Compute accuracy\n",
    "        test_accu.append(accuracy_score(batch_labels.detach().\n",
    "                                        numpy(),\n",
    "                                        batch_preds.detach().\n",
    "                                        numpy()))\n",
    "    test_loss = np.mean(test_loss)\n",
    "    test_accu = np.mean(test_accu)\n",
    "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90695c4c",
   "metadata": {},
   "source": [
    "## Train attention model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a18d4c",
   "metadata": {},
   "source": [
    "### Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = load_file(tokens_path)\n",
    "labels = load_file(labels_path)\n",
    "embeddings = load_file(embeddings_path)\n",
    "label_encoder = load_file(label_encoder_path)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "vocabulary = load_file(vocabulary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37271b2",
   "metadata": {},
   "source": [
    "### Split data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12905bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokens, labels,\n",
    "                                                    test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, \n",
    "                                                      y_train,\n",
    "                                                      test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca11ca1",
   "metadata": {},
   "source": [
    "### Create PyTorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train, embeddings, y_train)\n",
    "valid_dataset = TextDataset(X_valid, embeddings, y_valid)\n",
    "test_dataset = TextDataset(X_test, embeddings, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699aa78",
   "metadata": {},
   "source": [
    "### Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=16,\n",
    "                                           shuffle=True, \n",
    "                                           drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                           batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d484ef",
   "metadata": {},
   "source": [
    "### Create model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8cffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() \n",
    "                      else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionModel(vec_len, seq_len, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a7d38",
   "metadata": {},
   "source": [
    "### Move the model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942190dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1850d25e",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a405ca0",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de992e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, valid_loader, model, criterion, optimizer,\n",
    "      device, num_epochs, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520d07b",
   "metadata": {},
   "source": [
    "### Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, model, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb74fa",
   "metadata": {},
   "source": [
    "## Predict on new text\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b65a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''I am a victim of Identity Theft & currently have an Experian account that \n",
    "I can view my Experian Credit Report and getting notified when there is activity on \n",
    "my Experian Credit Report. For the past 3 days I've spent a total of approximately 9 \n",
    "hours on the phone with Experian. Every time I call I get transferred repeatedly and \n",
    "then my last transfer and automated message states to press 1 and leave a message and \n",
    "someone would call me. Every time I press 1 I get an automatic message stating than you \n",
    "before I even leave a message and get disconnected. I call Experian again, explain what \n",
    "is happening and the process begins again with the same end result. I was trying to have \n",
    "this issue attended and resolved informally but I give up after 9 hours. There are hard \n",
    "hit inquiries on my Experian Credit Report that are fraud, I didn't authorize, or recall \n",
    "and I respectfully request that Experian remove the hard hit inquiries immediately just \n",
    "like they've done in the past when I was able to speak to a live Experian representative \n",
    "in the United States. The following are the hard hit inquiries : BK OF XXXX XX/XX/XXXX \n",
    "XXXX XXXX XXXX  XX/XX/XXXX XXXX  XXXX XXXX  XX/XX/XXXX XXXX  XX/XX/XXXX XXXX  XXXX \n",
    "XX/XX/XXXX'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d0cc6",
   "metadata": {},
   "source": [
    "### Process input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d73be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = input_text.lower()\n",
    "input_text = re.sub(r\"[^\\w\\d'\\s]+\", \" \", input_text)\n",
    "input_text = re.sub(\"\\d+\", \"\", input_text)\n",
    "input_text = re.sub(r'[x]{2,}', \"\", input_text)\n",
    "input_text = re.sub(' +', ' ', input_text)\n",
    "tokens = word_tokenize(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['<pad>']*(20-len(tokens))+tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_token = []\n",
    "for token in tokens:\n",
    "    if token in vocabulary:\n",
    "        idx_token.append(vocabulary.index(token))\n",
    "    else:\n",
    "        idx_token.append(vocabulary.index('<unk>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_emb = embeddings[idx_token,:]\n",
    "token_emb = token_emb[:seq_len, :]\n",
    "inp = torch.from_numpy(token_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adfdbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.cat((torch.ones(inp.shape[0],1), inp), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76afacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() \n",
    "                      else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b78e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = inp.to(device)\n",
    "inp = torch.unsqueeze(inp, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = load_file(label_encoder_path)\n",
    "num_classes = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ea657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "model = AttentionModel(vec_len, seq_len, num_classes)\n",
    "\n",
    "# Load trained weights\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Move the model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    \n",
    "# Forward pass\n",
    "out = torch.squeeze(model(inp))\n",
    "\n",
    "# Find predicted class\n",
    "prediction = label_encoder.classes_[torch.argmax(out)]\n",
    "print(f\"Predicted  Class: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7392fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
